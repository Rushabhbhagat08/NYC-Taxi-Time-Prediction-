{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Rushabhbhagat08/NYC-Taxi-Time-Prediction-/blob/main/NYC_Taxi_Time_Prediction.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vncDsAP0Gaoa"
      },
      "source": [
        "# **Project Name**    - NYC Taxi Trip Time Prediction \n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "beRrZCGUAJYm"
      },
      "source": [
        "##### **Project Type**    - EDA/Regression\n",
        "##### **Contribution**    - Individual\n",
        "##### **Team Member**: Rushabh Anilrao Bhagat"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FJNUwmbgGyua"
      },
      "source": [
        "# **Project Summary -**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "F6v_1wHtG2nS"
      },
      "source": [
        "New York City taxi rides form the core of the traffic in the city of New York. The many rides taken every day by New Yorkers in the busy city can give us a great idea of traffic times, road blockages, and so on. Predicting the duration of a taxi trip is very important since a user would always like to know precisely how much time it would require of him to travel from one place to another. Given the rising popularity of app-based taxi usage through common vendors like Ola and Uber, competitive pricing has to be offered to ensure users choose them. Prediction of duration and price of trips can help users to plan their trips properly, thus keeping potential margins for traffic congestions. It can also help drivers to determine the correct route which in-turn will take lesser time as accordingly. Moreover, the transparency about pricing and trip duration will help to attract users at times when popular taxi app-based vendor services apply surge fares. Thus in this research study, we used real-time data which customers would provide at the start of a ride, or while booking a ride to predict the duration and fare. This data includes pickup and drop-off point coordinates, the distance of the trip, start time, number of passengers, and a rate code belonging to the different classes of cabs available such that the rate applied is based on a regular or airport basis. Hereafter, we applied multiple algorithm Perceptron models to find out which one of them provides better accuracy and relationships between real-time variables. At last, a comparison of the two mentioned algorithms facilitates us to decide that Random Forest is more fitter and efficient than Decision Tree Perceptron for taxi trip duration-based predictions."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "w6K7xa23Elo4"
      },
      "source": [
        "# **GitHub Link -**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "h1o69JH3Eqqn"
      },
      "source": [
        "Provide your GitHub Link here."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yQaldy8SH6Dl"
      },
      "source": [
        "# **Problem Statement**\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DpeJGUA3kjGy"
      },
      "source": [
        "Your task is to build a model that predicts the total ride duration of taxi trips in New York City. Your primary dataset is one released by the NYC Taxi and Limousine Commission, which includes pickup time, geo-coordinates, number of passengers, and several other variables."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "O_i_v8NEhb9l"
      },
      "source": [
        "# ***Let's Begin !***"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "v4yT_FOM1gF5"
      },
      "source": [
        "###**Install Requird Libraries**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "K1flaKOr1ydp"
      },
      "outputs": [],
      "source": [
        "!pip install haversine"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HhfV-JJviCcP"
      },
      "source": [
        "## ***1. Know Your Data***"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Y3lxredqlCYt"
      },
      "source": [
        "### Import Libraries"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "M8Vqi-pPk-HR"
      },
      "outputs": [],
      "source": [
        "# Import Libraries\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "%matplotlib inline\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "import datetime as dt\n",
        "# plt.style.use(\"dark_background\")\n",
        "# from sklearn.model_selection import GridSearchCV\n",
        "# from pandas_profiling import ProfileReport\n",
        "from haversine import haversine\n",
        "from sklearn.linear_model import LinearRegression\n",
        "from sklearn.ensemble import RandomForestRegressor\n",
        "# from xgboost import XGBRegressor\n",
        "# from sklearn import metrics\n",
        "from sklearn.model_selection import train_test_split, GridSearchCV\n",
        "from haversine import haversine\n",
        "# import statsmodels.formula.api as sm\n",
        "# from sklearn.model_selection import learning_curve\n",
        "# from sklearn.model_selection import ShuffleSplit\n",
        "import warnings; warnings.simplefilter('ignore')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3RnN4peoiCZX"
      },
      "source": [
        "### Dataset Loading"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "4CkvbW_SlZ_R"
      },
      "outputs": [],
      "source": [
        "#Load Drive\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "dshWkndLlJVl"
      },
      "outputs": [],
      "source": [
        "# Load Dataset\n",
        "Taxi_Time_df=pd.read_csv('/content/drive/MyDrive/Regression_project/NYC taxi Time Prediction /Copy of NYC Taxi Data.csv')\n",
        "# Taxi_Time_df=pd.read_csv('/NYC Taxi Data.csv')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "QWn324CFtM5O"
      },
      "outputs": [],
      "source": [
        "# Taxi_Time_df.hist()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "x71ZqKXriCWQ"
      },
      "source": [
        "### Dataset First View"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "LWNFOSvLl09H"
      },
      "outputs": [],
      "source": [
        "# Dataset First Look\n",
        "Taxi_Time_df.head()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "lqpe5n8olTBU"
      },
      "outputs": [],
      "source": [
        "Taxi_Time_df.tail()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7hBIi_osiCS2"
      },
      "source": [
        "### Dataset Rows & Columns count"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Kllu7SJgmLij"
      },
      "outputs": [],
      "source": [
        "# Dataset Rows & Columns count\n",
        "rows= Taxi_Time_df.shape[0]\n",
        "columns = Taxi_Time_df.shape[1]\n",
        "print(f\"The number of rows is {rows} and number of columns is {columns}.\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JlHwYmJAmNHm"
      },
      "source": [
        "### Dataset Information"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "e9hRXRi6meOf"
      },
      "outputs": [],
      "source": [
        "# Dataset Info\n",
        "Taxi_Time_df.info()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "35m5QtbWiB9F"
      },
      "source": [
        "#### Duplicate Values"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "1sLdpKYkmox0"
      },
      "outputs": [],
      "source": [
        "# Dataset Duplicate Value Count\n",
        "Taxi_Time_df['vendor_id'].value_counts()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PoPl-ycgm1ru"
      },
      "source": [
        "#### Missing Values/Null Values"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "GgHWkxvamxVg"
      },
      "outputs": [],
      "source": [
        "# Missing Values/Null Values Count\n",
        "Taxi_Time_df.isnull().sum()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OguxOcV3oRjS"
      },
      "source": [
        "There is no NaN/NULL record in the dataset, So we dont have to impute any record."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "H0kj-8xxnORC"
      },
      "source": [
        "### What did you know about your dataset?"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gfoNAAC-nUe_"
      },
      "source": [
        "* The dataset is based on the 2016 NYC Yellow Cab trip record data made available in Big Query on Google Cloud Platform. The data was originally published by the NYC Taxi and Limousine Commission (TLC). The data was sampled and cleaned for the purposes of this project. Based on individual trip attributes, you should predict the duration of each trip in the test set.\n",
        "NYC Taxi Data.csv - the training set (contains 1458644 trip records)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nA9Y7ga8ng1Z"
      },
      "source": [
        "## ***2. Understanding Your Variables***"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "j7xfkqrt5Ag5"
      },
      "outputs": [],
      "source": [
        "# Dataset Columns\n",
        "Taxi_Time_df.columns"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "DnOaZdaE5Q5t"
      },
      "outputs": [],
      "source": [
        "# Dataset Describe\n",
        "Taxi_Time_df.describe()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PBTbrJXOngz2"
      },
      "source": [
        "### Variables Description "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aJV4KIxSnxay"
      },
      "source": [
        "1. id - a unique identifier for each trip.\n",
        "\n",
        "2. vendor_id - a code indicating the provider associated with the trip record.\n",
        "\n",
        "3. pickup_datetime - date and time when the meter was engaged.\n",
        "\n",
        "4. dropoff_datetime - date and time when the meter was disengaged.\n",
        "\n",
        "5. passenger_count - the number of passengers in the vehicle (driver entered value).\n",
        "\n",
        "6. pickup_longitude - the longitude where the meter was engaged.\n",
        "\n",
        "7. pickup_latitude - the latitude where the meter was engaged.\n",
        "\n",
        "8. dropoff_longitude - the longitude where the meter was disengaged.\n",
        "\n",
        "9. dropoff_latitude - the latitude where the meter was disengaged.\n",
        "\n",
        "10. store_and_fwd_flag - This flag indicates whether the trip record was held     in vehicle memory before sending to the vendor because the vehicle did not have a connection to the server - Y=store and forward; N=not a store and forward trip.\n",
        "11. trip_duration - duration of the trip in seconds.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "u3PMJOP6ngxN"
      },
      "source": [
        "### Check Unique Values for each variable."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "zms12Yq5n-jE"
      },
      "outputs": [],
      "source": [
        "# Check Unique Values for each variable.\n",
        "print(\"There are %d unique id's in Training dataset, which is equal to the number of records\"%(Taxi_Time_df.id.nunique()))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dauF4eBmngu3"
      },
      "source": [
        "## 3. ***Data Wrangling***"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bKJF3rekwFvQ"
      },
      "source": [
        "### Data Wrangling Code"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "cM1CPrBQp4NR"
      },
      "outputs": [],
      "source": [
        "#Calculate and assign new columns to the dataframe such as weekday,\n",
        "#month and pickup_hour which will help us to gain more insights from the data.\n",
        "\n",
        "Taxi_Time_df['pickup_datetime'] = pd.to_datetime(Taxi_Time_df['pickup_datetime'])\n",
        "Taxi_Time_df['dropoff_datetime'] = pd.to_datetime(Taxi_Time_df['dropoff_datetime'])\n",
        "Taxi_Time_df['pickup_day'] = Taxi_Time_df['pickup_datetime'].dt.day\n",
        "Taxi_Time_df['pickup_month'] = Taxi_Time_df['pickup_datetime'].dt.month\n",
        "Taxi_Time_df['pickup_date'] = Taxi_Time_df['pickup_datetime'].dt.date\n",
        "Taxi_Time_df['pickup_hour'] = Taxi_Time_df['pickup_datetime'].dt.hour\n",
        "Taxi_Time_df['pickup_min'] = Taxi_Time_df['pickup_datetime'].dt.minute\n",
        "Taxi_Time_df['dropoff_min'] = Taxi_Time_df['dropoff_datetime'].dt.minute\n",
        "Taxi_Time_df['pickup_weekday'] = Taxi_Time_df['pickup_datetime'].dt.weekday \n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "JlKhV4gupZyL"
      },
      "outputs": [],
      "source": [
        "Taxi_Time_df['store_and_fwd_flag']=Taxi_Time_df['store_and_fwd_flag'].apply(lambda x : 0 if x=='N' else 1)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "TfI33luwEpOi"
      },
      "outputs": [],
      "source": [
        "Taxi_Time_df.head()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "kEPA1jBDFla_"
      },
      "outputs": [],
      "source": [
        "#calc_distance is a function to calculate distance between pickup and dropoff coordinates using Haversine formula.\n",
        "def calc_distance(df):\n",
        "    pickup = (df['pickup_latitude'], df['pickup_longitude'])\n",
        "    drop = (df['dropoff_latitude'], df['dropoff_longitude'])\n",
        "    return haversine(pickup, drop)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Z4gNeKayHim6"
      },
      "outputs": [],
      "source": [
        "#Calculate distance and assign new column to the dataframe.\n",
        "Taxi_Time_df['distance'] = Taxi_Time_df.apply(lambda x: calc_distance(x), axis = 1)\n",
        "# Taxi_Time_df['distance']"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "MtunjeYKH2Pt"
      },
      "outputs": [],
      "source": [
        "#Calculate Speed in km/h for further insights\n",
        "Taxi_Time_df['speed'] = (Taxi_Time_df.distance/(Taxi_Time_df.trip_duration/3600))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "U3HFGFq3H3SH"
      },
      "outputs": [],
      "source": [
        "#Check the type of each variable\n",
        "Taxi_Time_df.dtypes.reset_index()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "t9HC8urtRipa"
      },
      "outputs": [],
      "source": [
        "#Dummify all the categorical features like \"store_and_fwd_flag, vendor_id, month, weekday_num, pickup_hour, passenger_count\" except the label i.e. \"trip_duration\"\n",
        "\n",
        "dummy = pd.get_dummies(Taxi_Time_df.store_and_fwd_flag, prefix='flag')\n",
        "dummy.drop(dummy.columns[0], axis=1, inplace=True) #avoid dummy trap\n",
        "data = pd.concat([Taxi_Time_df,dummy], axis = 1)\n",
        "\n",
        "dummy = pd.get_dummies(Taxi_Time_df.vendor_id, prefix='vendor_id')\n",
        "dummy.drop(dummy.columns[0], axis=1, inplace=True) #avoid dummy trap\n",
        "data = pd.concat([Taxi_Time_df,dummy], axis = 1)\n",
        "\n",
        "dummy = pd.get_dummies(Taxi_Time_df.pickup_month, prefix='pickup_month')\n",
        "dummy.drop(dummy.columns[0], axis=1, inplace=True) #avoid dummy trap\n",
        "data = pd.concat([Taxi_Time_df,dummy], axis = 1)\n",
        "\n",
        "dummy = pd.get_dummies(Taxi_Time_df.pickup_weekday, prefix='pickup_weekday')\n",
        "dummy.drop(dummy.columns[0], axis=1, inplace=True) #avoid dummy trap\n",
        "data = pd.concat([Taxi_Time_df,dummy], axis = 1)\n",
        "\n",
        "dummy = pd.get_dummies(Taxi_Time_df.pickup_hour, prefix='pickup_hour')\n",
        "dummy.drop(dummy.columns[0], axis=1, inplace=True) #avoid dummy trap\n",
        "data = pd.concat([Taxi_Time_df,dummy], axis = 1)\n",
        "\n",
        "dummy = pd.get_dummies(Taxi_Time_df.passenger_count, prefix='passenger_count')\n",
        "dummy.drop(dummy.columns[0], axis=1, inplace=True) #avoid dummy trap\n",
        "data = pd.concat([Taxi_Time_df,dummy], axis = 1)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "RNU6Yn_972Gi"
      },
      "outputs": [],
      "source": [
        "# Taxi_Time_df['pickup_day'].value_counts()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "WRx5VhIMRW1R"
      },
      "outputs": [],
      "source": [
        "#update a dataset\n",
        "Taxi_Time_df.head()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jofNLs1_MyYL"
      },
      "source": [
        "* Now our dataset is complete for the further analysis before we train our model with optimal variables"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GNCClz6squbf"
      },
      "source": [
        "## ***4. Data Vizualization, Storytelling & Experimenting with charts : Understand the relationships between variables***"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MnVwsG8VpEFK"
      },
      "source": [
        "# 1. <u>**`Univariate Analysis`**</u>\n",
        "\n",
        "---\n",
        "\n",
        "\n",
        "\n",
        "---\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0zOQYgCRmw9e"
      },
      "source": [
        "##**1. <u>Passengers</u>**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MSa1f5Uengrz"
      },
      "source": [
        "### What all manipulations have you done and insights you found?"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LbyXE7I1olp8"
      },
      "source": [
        "New York City Taxi Passenger Limit says:\n",
        "\n",
        "* A maximum of 4 passengers can ride in traditional cabs, there are also 5 passenger cabs that look more like minivans.\n",
        "\n",
        "* A child under 7 is allowed to sit on a passenger's lap in the rear seat in addition to the passenger limit.\n",
        "So, in total we can assume that maximum 6 passenger can board the new york taxi i.e. 5 adult + 1 minor"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "KJFNMygvj9Lr"
      },
      "outputs": [],
      "source": [
        "pd.options.display.float_format = '{:.2f}'.format #To suppres scientific notation.\n",
        "Taxi_Time_df.passenger_count.value_counts()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_5tfpjWIkHAe"
      },
      "outputs": [],
      "source": [
        "plt.figure(figsize = (20,5))\n",
        "sns.boxplot(data.passenger_count)\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kTYnrtdeXN-i"
      },
      "source": [
        "##### 1. Why did you pick the specific chart?"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DV5spzuHkNl2"
      },
      "source": [
        "Beacues, There are some trips with 0 passenger count.\n",
        "Few trips consisted of even 7, 8 or 9 passengers. so,\n",
        "Clear outliers and pointers to data inconsistency\n",
        "Most of trip consist of passenger either 1 or 2. thats reason we use boxplot here.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wogTVumJXiBg"
      },
      "source": [
        "2. What is/are the insight(s) found from the chart?"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8cPhVduTXlbk"
      },
      "source": [
        "Passenger count is a driver entered value. Since the trip is not possible without passengers. It is evident that the driver forgot to enter the value for the trips with 0 passenger count. Lets analyze the passenger count distribution further to make it consistent for further analysis"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "K9RApUp1keR5"
      },
      "outputs": [],
      "source": [
        "data.passenger_count.describe()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "efP9rXVkkoJo"
      },
      "source": [
        "As per above details. Mean median and mode are all approx equal to 1. So we would replace the 0 passenger count with 1."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "MDcvr3g8knxa"
      },
      "outputs": [],
      "source": [
        "data['passenger_count']=data.passenger_count.map(lambda x:1 if x==0 else x)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FPet186VlS_1"
      },
      "source": [
        "Also, we will remove the records with passenger count > 7, 8 or 9 as they are extreme values and looks very odd to be ocupied in a taxi."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "hjKrCJDslXYD"
      },
      "outputs": [],
      "source": [
        "data=data[data.passenger_count<=6]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "du1xCLo6lmTV"
      },
      "source": [
        "Now the data is consistent with respect to the passenger count. Let's take a look at the ditribution with a graph below"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "WMAAkUTalqsL"
      },
      "outputs": [],
      "source": [
        "sns.countplot(data.passenger_count)\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JHe3z6UHl6Zt"
      },
      "source": [
        "now, you observe the graph It is evident that most of the trips was taken by single passenger."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vARJV3-Xm6qV"
      },
      "source": [
        "##**2. <u>Vendor</u>**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Cd7p5SFsm_w4"
      },
      "source": [
        "Here we analyze taxi data only for the 2 vendors which are listed as 1 and 2 in the datset."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "sKUmQvaum-o0"
      },
      "outputs": [],
      "source": [
        "sns.countplot(data.vendor_id)\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JG28vO9mnHxX"
      },
      "source": [
        "Though both the vendors seems to have almost equal market share. But Vendor 2 is evidently more famous among the population as per the above graph."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "347v2gcwnM3i"
      },
      "source": [
        "##**3. <u>Distance</u>**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tH__p4kynN1U"
      },
      "source": [
        "\n",
        "Let's now have a look on the distribution of the distance across the different types of rides."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "vUSWtZ-Anirh"
      },
      "outputs": [],
      "source": [
        "print(data.distance.describe())"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3Wkwp3Axnsuh"
      },
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "qCyyDpttnaKd"
      },
      "outputs": [],
      "source": [
        "plt.figure(figsize=(20,5))\n",
        "sns.boxplot(data.distance)\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "u2SR1j4Eoqg2"
      },
      "source": [
        "##### 1. What is/are the insight(s) found from the chart?\n",
        "\n",
        "There some trips with over 100 km distance. And \n",
        "some of the trips distance value is 0 km."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0HG0xI4Zo8mi"
      },
      "source": [
        "So, the mean distance travelled is approx 3.5 kms.\n",
        "standard deviation of 4.3 which shows that most of the trips are limited to the range of 1-10 kms."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "pKyVfDTSpFQ5"
      },
      "outputs": [],
      "source": [
        "print(\"There are {} trip records with 0 km distance\".format(data.distance[data.distance == 0 ].count()))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "CjMxAC3ApJX7"
      },
      "outputs": [],
      "source": [
        "data[data.distance == 0 ].head()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kw2zHVHLqC9I"
      },
      "source": [
        "Around 6K trip record with distance equal to 0. Below are some possible explanation for such records.\n",
        "1. Customer changed mind and cancelled the journey just after accepting it.\n",
        "2. Software didn't recorded dropoff location properly due to which dropoff location is the same as the pickup location.\n",
        "3. Issue with GPS tracker while the journey is being finished.\n",
        "4. Driver cancelled the trip just after accepting it due to some reason. So the trip couldn't start\n",
        "Or some other issue with the software itself which a technical guy can explain\n",
        "There is some serious inconsistencies in the data where drop off location is same as the pickup location. \n",
        "\n",
        "We can't think off imputing the distance values considering a correlation with the duration because the dropoff_location coordinates would not be inline with the distance otherwise."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "EncNnAlvpgT1"
      },
      "outputs": [],
      "source": [
        "data.distance.groupby(pd.cut(data.distance, np.arange(0,100,10))).count().plot(kind='barh')\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DgDUOMQkpm1u"
      },
      "source": [
        "From the above observation it is evident that most of the rides are completed between 1-10 Kms with some of the rides with distances between 10-30 kms. Other slabs bar are not visible because the number of trips are very less as compared to these slabs"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JPPW9WqIqnTX"
      },
      "source": [
        "##**4. <u>Trip duration</u>**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "WWypsVF3qqkM"
      },
      "outputs": [],
      "source": [
        "data.trip_duration.describe()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "WAgNdO6Dq1rK"
      },
      "outputs": [],
      "source": [
        "plt.figure(figsize = (20,5))\n",
        "sns.boxplot(data.trip_duration)\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ngyRFZaLYyqU"
      },
      "source": [
        "##### 1. Why did you pick the specific chart?\n",
        "\n",
        "Some trip durations are over 100000 seconds which are clear outliers and should be removed. so we use boxplot here."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7hjpWUii7eAt"
      },
      "source": [
        "###1. What is/are the insight(s) found from the chart?\n",
        "There are some durations with as low as 1 second. which points towards trips with 0 km distance.\n",
        "Major trip durations took between 10-20 mins to complete.\n",
        "Mean and mode are not same which shows that trip duration distribution is skewed towards right."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "elIJ38ANq-Nr"
      },
      "outputs": [],
      "source": [
        "data.trip_duration.groupby(pd.cut(data.trip_duration, np.arange(1,max(data.trip_duration),3600))).count()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tJFyrwYYG5BG"
      },
      "source": [
        "There are some trips with more than 24 hours of travel duration i.e. 86400 seconds. Which might have occured on weekends for the outstation travels.\n",
        "Major chunk of trips are completed within an interval of 1 hour with some good numbers of trips duration going above 1 hour.\n",
        "Let's look at those trips with huge duration, these are outliers and should be removed for the data consistency."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ues2_z-9HCRH"
      },
      "outputs": [],
      "source": [
        "data[data.trip_duration > 86400]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rZ0mYXT9HSZV"
      },
      "source": [
        "These trips run for more than 20 days, which seems unlikely by the distance travelled.\n",
        "All the trips are taken by vendor 1 which points us to the fact that this vendor might allows much longer trip for outstations.\n",
        "All these trips are either taken on Tuesday's in 1st month or Saturday's in 2nd month. There might be some relation with the weekday, pickup location, month and the passenger.\n",
        "But they fail our purpose of correct prediction and bring inconsistencies in the algorithm calculation."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "lBunZCQqHUAA"
      },
      "outputs": [],
      "source": [
        "data[data.trip_duration <= 86400]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Nl1dDDqhHaPU"
      },
      "source": [
        "Let's visualize the number of trips taken in slabs of 0-10, 20-30 ... minutes respectively"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "R4__cVPjHgtu"
      },
      "outputs": [],
      "source": [
        "data.trip_duration.groupby(pd.cut(data.trip_duration, np.arange(1,7200,600))).count().plot(kind='barh')\n",
        "plt.xlabel('Trip Counts')\n",
        "plt.ylabel('Trip Duration (seconds)')\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mRrZwlT_Hhv0"
      },
      "source": [
        "We can observe that most of the trips took 0 - 30 mins to complete i.e. approx 1800 secs. Let's move ahead to next feature."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "j-5yxz06r0ML"
      },
      "source": [
        "##**5. <u>Speed of a car</u>**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FLNCiYFVHqd3"
      },
      "source": [
        "Speed is a function of distance and time. Let's visualize speed in different trips.\n",
        "\n",
        "Maximum speed limit in NYC is as follows:\n",
        "\n",
        "25 mph in urban area i.e. 40 km/h\n",
        "65 mph on controlled state highways i.e. approx 104 km/h"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "p4g7z8TwHBlP"
      },
      "outputs": [],
      "source": [
        "data.speed.describe()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "yvjVCdgUHuJ8"
      },
      "outputs": [],
      "source": [
        "plt.figure(figsize = (20,5))\n",
        "sns.boxplot(data.speed)\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pOOFUd6IHw0N"
      },
      "source": [
        "###1. What is/are the insight(s) found from the chart?\n",
        "\n",
        "Many trips were done at a speed of over 200 km/h. Going SuperSonic..!!\n",
        "Let's remove them and focus on the trips which were done at less than 104 km/h as per the speed limits"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "gvfL7KqdIGxO"
      },
      "outputs": [],
      "source": [
        "data = data[data.speed <= 104]\n",
        "plt.figure(figsize = (20,5))\n",
        "sns.boxplot(data.speed)\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eDeo1ll6IGg-"
      },
      "source": [
        "Trips over 30 km/h are being considered as outliers but we cannot ignore them because they are well under the highest speed limit of 104 km/h on state controlled highways.\n",
        "Mostly trips are done at a speed range of 10-20 km/h with an average speed of around 14 km/h.\n",
        "Let's take a look at the speed range ditribution with the help of graph."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "kK48oXa5ILYT"
      },
      "outputs": [],
      "source": [
        "data.speed.groupby(pd.cut(data.speed, np.arange(0,104,10))).count().plot(kind = 'barh')\n",
        "plt.xlabel('Trip count')\n",
        "plt.ylabel('Speed (Km/H)')\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4jb8ZX2XHAtI"
      },
      "source": [
        "It is evident from this graph what we thought off earlier i.e. most of the trips were done at a speed range of 10-20 km/H."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "I9E2iOZ3rr8d"
      },
      "source": [
        "##**6. <u>Store_and_fwd_flag</u>**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dEKUfrKhIcB4"
      },
      "source": [
        "This flag indicates whether the trip record was held in vehicle memory before sending to the vendor because the vehicle did not have a connection to the server - Y=store and forward; N=not a store and forward trip."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "99a6E4_DIckj"
      },
      "outputs": [],
      "source": [
        "\n",
        "x=Taxi_Time_df['store_and_fwd_flag'].value_counts()\n",
        "print(x)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "zLiBq-hSCujA"
      },
      "outputs": [],
      "source": [
        "plt.style.use(\"classic\")\n",
        "plt.figure(figsize=(8,8))\n",
        "plt.pie(x, colors=['lightgreen', 'lightcoral'], shadow=True, explode=[0.5,0], autopct='%1.2f%%', startangle=200)\n",
        "plt.legend(labels=['Y','N'])\n",
        "plt.title(\"Store and Forward Flag\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fn8ZIOZbIh8U"
      },
      "source": [
        "* store_and_fwd_flag - This flag indicates whether the trip record was held in vehicle memory before sending to the vendor because the vehicle did not have a connection to the server - Y = store and forward; N = not a store and forward trip.\n",
        "\n",
        "* Visualization tells us that there were very few trips of which the records were stored in memory due to no connection to the server."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rBf6eHnwrbtE"
      },
      "source": [
        "#2. **<U>`Bivariate Analysis`</u>**\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Mx7ajVLDr_3S"
      },
      "source": [
        "## 1. **Total Trips**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IR1v6FsVrmHS"
      },
      "source": [
        "* **Total trips Per Hour**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QAIRW2odKZX2"
      },
      "source": [
        "Let's take a look at the distribution of the pickups across the 24 hour time scale."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "y_Vnv4XuKPnt"
      },
      "outputs": [],
      "source": [
        "sns.countplot(data.pickup_hour)\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6emelEusKbSG"
      },
      "source": [
        "It's inline with the general trend of taxi pickups which starts increasing from 6AM in the morning and then declines from late evening i.e. around 8 PM. There is no unusual behavior here."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8CH4dnuPrhQ2"
      },
      "source": [
        "* **Total trips per weekday**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "LcY0NcL7Kfas"
      },
      "outputs": [],
      "source": [
        "# changes in graph monday to friday\n",
        "plt.figure(figsize = (8,6))\n",
        "sns.countplot(data.pickup_weekday,palette='Accent')\n",
        "plt.xlabel(' Month ')\n",
        "# plt.ylabel('Pickup counts')\n",
        "plt.xticks([0,1,2,3,4,5,6], labels=['Mon','Tue','Wed','Thrus','Fri','Sat','Sun'], rotation=90)\n",
        "plt.show()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8rGdMSKQK7ve"
      },
      "source": [
        "Here we can see an increasing trend of taxi pickups starting from Monday till Friday. The trend starts declining from saturday till monday which is normal where some office going people likes to stay at home for rest on the weekends."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zdZIoI4Dhwhq"
      },
      "source": [
        "* **Total Trips Per Hours**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "7Bth2EMULkEA"
      },
      "outputs": [],
      "source": [
        "#changes in graph here\n",
        "sns.countplot(data.pickup_hour)\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iUCNHvL8h0cU"
      },
      "source": [
        "It's inline with the general trend of taxi pickups which starts increasing from 6AM in the morning and then declines from late evening i.e. around 8 PM. There is no unusual behavior here."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WLrOR636Ltub"
      },
      "source": [
        "###1. Interesting find:\n",
        "\n",
        "\n",
        "Taxi pickups increased in the late night hours over the weekend possibly due to more outstation rides or for the late night leisures nearby activities.\n",
        "Early morning pickups i.e before 5 AM have increased over the weekend in comparison to the office hours pickups i.e. after 7 AM which have decreased due to obvious reasons.\n",
        "Taxi pickups seems to be consistent across the week at 15 Hours i.e. at 3 PM."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VVsJTrCNrPQ4"
      },
      "source": [
        "* **Total trips per month**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jc5aUDc9tqnB"
      },
      "source": [
        "\n",
        "Let's take a look at the trip distribution across the months to understand if there is any diffrence in the taxi pickups in different months"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "proZAVpcrUVA"
      },
      "outputs": [],
      "source": [
        "#changes in month here\n",
        "# plt.figure(figsize=(10,10))\n",
        "sns.countplot(data.pickup_month,palette='Accent')\n",
        "plt.ylabel('Trip Counts')\n",
        "# plt.xlabel('Months')\n",
        "# plt.show()\n",
        "plt.xticks([0,1,2,3,4,5,6], labels=['Jan','Feb','March','April','May','June'], rotation=90)\n",
        "plt.title('Overall Monthly trips')\n",
        "\n",
        "\n",
        "# plt.style.use(\"dark_background\")\n",
        "# sns.countplot(Taxi_Time_df['month'], )\n",
        "#"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zja02wqZs1w2"
      },
      "source": [
        "##2. **Trip Duration**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-f60TwaAuECt"
      },
      "source": [
        "* **Trip Duration Per Hours**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cOvm50A-ukBP"
      },
      "source": [
        "We need to aggregate the total trip duration to plot it agaist the month. The aggregation measure can be anything like sum, mean, median or mode for the duration. Since we already did the outlier analysis, so we can take the mean to visualize the pattern which should not result in the bias of the general trend."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "fTwlhDbjyh0n"
      },
      "outputs": [],
      "source": [
        "group1 = data.groupby('pickup_hour').trip_duration.mean()\n",
        "sns.pointplot(group1.index, group1.values)\n",
        "plt.ylabel('Trip Duration (seconds)')\n",
        "plt.xlabel('Pickup Hour')\n",
        "plt.show()\n",
        "\n",
        "# group3 = data.groupby('month').trip_duration.mean()\n",
        "# sns.pointplot(group3.index, group3.values)\n",
        "# plt.ylabel('Trip Duration (seconds)')\n",
        "# plt.xlabel('Month')\n",
        "# plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OR6SlLxZapTZ"
      },
      "source": [
        "\n",
        "1. What is/are the insight(s) found from the chart?\n",
        "\n",
        "Average trip duration is lowest at 6 AM when there is minimal traffic on the roads.\n",
        "\n",
        "Average trip duration is generally highest around 3 PM during the busy streets.\n",
        "\n",
        "Trip duration on an average is similar during early morning hours i.e. before 6 AM & late evening hours i.e. after 6 PM."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "F2XatLb3bs_7"
      },
      "source": [
        "* **Trip duration per weekday**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "xscKWhLQyz9h"
      },
      "outputs": [],
      "source": [
        "group2 = data.groupby('pickup_weekday').trip_duration.mean()\n",
        "sns.pointplot(group2.index, group2.values)\n",
        "plt.ylabel('Trip Duration (seconds)')\n",
        "plt.xlabel('Weekday')\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hfej8wVWbC6I"
      },
      "source": [
        "We can see that trip duration is almost equally distributed across the week on a scale of 0-1000 minutes with minimal difference in the duration times. Also, it is observed that trip duration on thursday is longest among all days."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OUkXFKcycWd7"
      },
      "source": [
        "* **Trip duration per month**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "TuN15hmJzA6G"
      },
      "outputs": [],
      "source": [
        "group3 = data.groupby('pickup_month').trip_duration.mean()\n",
        "sns.pointplot(group3.index, group3.values)\n",
        "plt.ylabel('Trip Duration (seconds)')\n",
        "plt.xlabel('Month')\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nOZ41kiBcseo"
      },
      "source": [
        "We can see an increasing trend in the average trip duration along with each subsequent month.\n",
        "\n",
        "The duration difference between each month is not much. It has increased gradually over a period of 6 months.\n",
        "\n",
        "It is lowest during february when winters starts declining.\n",
        "\n",
        "There might be some seasonal parameters like wind/rain which can be a factor of this gradual increase in trip duration over a period. Like May is generally the considered as the wettest month in NYC and which is inline with our visualization. As it generally takes longer on the roads due to traffic jams during rainy season. So natually the trip duration would increase towards April May and June."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fup763HXc4e-"
      },
      "source": [
        "* **Trip duration per vendor**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Z8LyF29ZzM7-"
      },
      "outputs": [],
      "source": [
        "group4 = data.groupby('vendor_id').trip_duration.mean()\n",
        "sns.barplot(group4.index, group4.values)\n",
        "plt.ylabel('Trip Duration (seconds)')\n",
        "plt.xlabel('Vendor')\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "chOF1dg5dKuO"
      },
      "source": [
        "Vendor 2 takes the crown. Average trip duration for vendor 2 is higher than vendor 1 by approx 200 seconds i.e. atleast 3 minutes per trip."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "b8BPf-om09Qn"
      },
      "outputs": [],
      "source": [
        "# plt.figure(figsize = (6,6))\n",
        "# plot_dist = data.loc[(data.distance < 100)]\n",
        "# sns.boxplot(x = \"flag_Y\", y = \"distance\", data = plot_dist)\n",
        "# plt.ylabel('Distance (km)')\n",
        "# plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DCxaeYd5dVEK"
      },
      "source": [
        "##3. **Distance**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "znrRAja_mCkU"
      },
      "source": [
        "* **Distance per hour**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QJisHxRwmFxL"
      },
      "source": [
        "Now, let us check how the distance is distributed against different variables. We know that trip distance must be more or less proportional to the trip duration if we ignore general traffic and other stuff on the road. Let's visualize this for each hour now."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "q_Fl0lMv1Ewu"
      },
      "outputs": [],
      "source": [
        "# \n",
        "group5 = data.groupby('pickup_min').distance.mean()\n",
        "sns.pointplot(group5.index, group5.values)\n",
        "plt.ylabel('Distance (km)')\n",
        "plt.show()\n",
        "#plt.scatter(data.trip_duration, data.distance , s=1, alpha=0.5)\n",
        "# plt.ylabel('Distance')\n",
        "# plt.xlabel('Trip Duration')\n",
        "# plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GF8Ens_Soomf"
      },
      "source": [
        "Trip distance is highest during early morning hours which can account for some things like:\n",
        "\n",
        "Outstation trips taken during the weekends.\n",
        "\n",
        "Longer trips towards the city airport which is located in the outskirts of the city.\n",
        "\n",
        "Trip distance is fairly equal from morning till the evening varying around 3 - 3.5 kms.\n",
        "\n",
        "It starts increasing gradually towards the late night hours starting from evening till 5 AM and decrease steeply towards morning."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "N3_R_FqJm519"
      },
      "source": [
        "* **Distance per weekday**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "c3so3hI6487q"
      },
      "outputs": [],
      "source": [
        "group6 = data.groupby('pickup_weekday').distance.mean()\n",
        "sns.pointplot(group6.index, group6.values)\n",
        "plt.ylabel('Distance (km)')\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "i7jIMHXhnN6m"
      },
      "source": [
        "So it's a fairly equal distribution with average distance metric verying around 3.5 km/h with Sunday being at the top may be due to outstation trips or night trips towards the airport."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kH4rh2I5nWLc"
      },
      "source": [
        "* **Distance per month**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "7v_ESjsspbW7"
      },
      "outputs": [],
      "source": [
        "# Chart - 1 visualization code\n",
        "group7 = data.groupby('pickup_month').distance.mean()\n",
        "sns.pointplot(group7.index, group7.values)\n",
        "plt.ylabel('Distance (km)')\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "K5QZ13OEpz2H"
      },
      "source": [
        "Here also the distibution is almost equivalent, varying mostly around 3.5 km/h with 5th month being the highest in the average distance and 2nd month being the lowest."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lJwKyU3SnmR4"
      },
      "source": [
        "* **Distance Per Vendor**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "gLij0fgwnm2j"
      },
      "outputs": [],
      "source": [
        "group8 = data.groupby('vendor_id').distance.mean()\n",
        "sns.barplot(group8.index, group8.values)\n",
        "plt.ylabel(\"Distance km\")\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XESiWehPqBRc"
      },
      "source": [
        "This is more or less same picture with both the vendors. Nothing more to analyze in this."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XfBXq5iSn-s2"
      },
      "source": [
        "##4. **Average**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0iPkzszcn5VY"
      },
      "source": [
        "* **Average speed per hour**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7y82aJWRoQXI"
      },
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "nO_84WhdoPBe"
      },
      "outputs": [],
      "source": [
        "group9 = data.groupby('pickup_hour').speed.mean()\n",
        "sns.pointplot(group9.index, group9.values)\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "L7vW7aCWoU50"
      },
      "source": [
        "The average trend is totally inline with the normal circumstances.\n",
        "\n",
        "Average speed tend to increase after late evening and continues to increase gradually till the late early morning hours.\n",
        "\n",
        "Average taxi speed is highest at 5 AM in the morning, then it declines steeply as the office hours approaches.\n",
        "\n",
        "Average taxi speed is more or less same during the office hours i.e. from 8 AM till 6PM in the evening."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XKKWD6LmocgB"
      },
      "source": [
        "* **Average speed Per Weekday**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "0T4GVdu-ojel"
      },
      "outputs": [],
      "source": [
        "group10 = data.groupby('pickup_weekday').speed.mean()\n",
        "sns.pointplot(group10.index, group10.values)\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "E1sS_k7boyEV"
      },
      "source": [
        "Average taxi speed is higher on weekend as compared to the weekdays which is obvious when there is mostly rush of office goers and business owners.\n",
        "\n",
        "Even on monday the average taxi speed is shown higher which is quite surprising when it is one of the most busiest day after the weekend. There can be several possibility for such behaviour\n",
        "Lot of customers who come back from outstation in early hours of Monday before 6 AM to attend office on time.\n",
        "\n",
        "Early morning hours customers who come from the airports after vacation to attend office/business on time for the coming week.\n",
        "\n",
        "There could be some more reasons as well which only a local must be aware of.\n",
        "We also can't deny the anomalies in the dataset. which is quite cumbersome to spot in such a large dataset."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NC_X3p0fY2L0"
      },
      "source": [
        "#### Chart - 14 - Correlation Heatmap"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "xyC9zolEZNRQ"
      },
      "outputs": [],
      "source": [
        "# Correlation Heatmap visualization code\n",
        "#Plotting Pearson Correlation heatmap\n",
        "\n",
        "plt.figure(figsize=(20,10))\n",
        "sns.heatmap(Taxi_Time_df.corr()*100, annot=True, cmap='inferno')\n",
        "plt.title('Correlation Plot')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yLjJCtPM0KBk"
      },
      "source": [
        "## ***6. Feature Engineering & Data Pre-processing***"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uDIHFwk3p54S"
      },
      "source": [
        "After looking at the dataset from different perspectives. Let's prepare our dataset before training our model. Since our dataset do not contain very large number of dimensions. We will first try to use feature selection instead of the feature extraction technique."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "TOv4dX8E8izK"
      },
      "outputs": [],
      "source": [
        "#Encode your categorical columns\n",
        "Taxi_Time_df.columns"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "0ErzOAs9Xz0-"
      },
      "outputs": [],
      "source": [
        "#show the all columns \n",
        "Taxi_Time_df.head()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "wvtypWbp-D1n"
      },
      "outputs": [],
      "source": [
        "# #dropping unwanted columns\n",
        "Taxi_Time_df = Taxi_Time_df.drop(['id','pickup_datetime','pickup_date','dropoff_datetime'], axis=1)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8_XNzTNGlGH0"
      },
      "source": [
        "##**Normalization**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "hGRGl2Jlumup"
      },
      "outputs": [],
      "source": [
        "#Predictors and Target Variable\n",
        "\n",
        "X = Taxi_Time_df.drop(['trip_duration'], axis=1)\n",
        "y = np.log(Taxi_Time_df['trip_duration'])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "avMFkE_WJAWw"
      },
      "outputs": [],
      "source": [
        "X.head()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "osujEyQNJKZW"
      },
      "outputs": [],
      "source": [
        "# Normalising Predictors and creating new dataframe\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "\n",
        "cols = X.columns\n",
        "\n",
        "ss = StandardScaler()\n",
        "\n",
        "new_df = ss.fit_transform(X)\n",
        "new_df = pd.DataFrame(new_df, columns=cols)\n",
        "new_df.head()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8t96_I4Nlige"
      },
      "source": [
        "Normalizing the Dataset using Standard Scaling Technique.\n",
        "\n",
        "Now, Why Standard Scaling ? Why not MinMax or Normalizer ?\n",
        "\n",
        "* It is because MinMax adjusts the value between 0s and 1s , which tend to work better for optimization techniques like Gradient descent and machine learning algorithms like KNN.\n",
        "\n",
        "* While, Normalizer uses distance measurement like Euclidean or Manhattan, so Normalizer tend to work better with KNN."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wuT-QGieldib"
      },
      "source": [
        "## **The First Approach - Decomposition using Principal Component Analysis (PCA)**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SR1Hj1-tlmn_"
      },
      "source": [
        "* Now that were done, we have to pass our Scaled Dataframe in PCA model and observe the elbow plot to get better idea of explained variance.\n",
        "\n",
        "* Why PCA ? It's a Dimensionality Reduction Technique. It is also a Feature extraction Technique. By PCA we create new features from old (Original) Features but the new features will always be independent of each other. So, its not just Dimensionality Reduction Process, we are even eliminating Correlation between the Variables.\n",
        "\n",
        "* We'll also go through a approach without using PCA in Second Part and Later compare results with PCA approach."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "l1U7zvZHl2GE"
      },
      "outputs": [],
      "source": [
        "X = new_df"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "jtAnFO50l78u"
      },
      "outputs": [],
      "source": [
        "#Applying PCA\n",
        "\n",
        "from sklearn.decomposition import PCA\n",
        "\n",
        "pca = PCA(n_components=len(Taxi_Time_df.columns)-1)\n",
        "pca.fit_transform(X)\n",
        "var_rat = pca.explained_variance_ratio_\n",
        "var_rat"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "5TeSLk5amENV"
      },
      "outputs": [],
      "source": [
        "#Variance Ratio vs PC plot\n",
        "\n",
        "plt.figure(figsize=(15,6))\n",
        "plt.bar(np.arange(pca.n_components_), pca.explained_variance_, color=\"grey\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pmH93cFjmJ1a"
      },
      "source": [
        "* At 14th component our PCA model seems to go Flat without explaining much of a Variance."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "lK0PyPXamMVe"
      },
      "outputs": [],
      "source": [
        "#Cumulative Variance Ratio\n",
        "\n",
        "plt.figure(figsize=(10,6))\n",
        "plt.plot(np.cumsum(var_rat)*100, color=\"g\", marker='o')\n",
        "plt.xlabel(\"Principal Components\")\n",
        "plt.ylabel(\"Cumulative Variance Ratio\")\n",
        "plt.title('Elbow Plot')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "lYOpIKSQmR2Y"
      },
      "outputs": [],
      "source": [
        "#Applying PCA as per required components\n",
        "\n",
        "pca = PCA(n_components=14)\n",
        "transform = pca.fit_transform(X)\n",
        "pca.explained_variance_"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YgyZjEoDmYa0"
      },
      "source": [
        "* Above , we had considered 12 as a required number of components and extracted new features by transforming the Data."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "j-BBmiYemc9A"
      },
      "outputs": [],
      "source": [
        "#importance of features in Particular Principal Component\n",
        "\n",
        "plt.figure(figsize=(25,6))\n",
        "sns.heatmap(pca.components_, annot=True, cmap=\"winter\")\n",
        "plt.ylabel(\"Components\")\n",
        "plt.xlabel(\"Features\")\n",
        "plt.xticks(np.arange(len(X.columns)), X.columns, rotation=90)\n",
        "plt.title('Contribution of a Particular feature to our Principal Components')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dibmhnq4mt3Z"
      },
      "source": [
        "* Above plot gives us detailed idealogy of which feature has contributed more or less to our each Principal Component.\n",
        "\n",
        "* Pricipal Components are our new features which consists of Information from every other original Feature we have.\n",
        "\n",
        "* We reduce the Dimensions using PCA by retaining as much as Information possible."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6ysQeJ_bm2AU"
      },
      "source": [
        "###**Splitting Data and Choosing Algorithms**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-yLRJQuWm8BK"
      },
      "source": [
        "Lets pass the PCA Transformed data in our Machine Learning Regression Algorithms. To begin with , Linear Regression is a good approach, by splitting our Data into Training and Testing (30%)."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TsAJyGSdnLQq"
      },
      "source": [
        "# ***7. ML Model Implementation***"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5eZmBrCInDkU"
      },
      "source": [
        "### **Why Linear Regression , Decision Tree and Random Forest ?**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FpmNmke_nHtX"
      },
      "source": [
        "**Linear Regression**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "07tP5NsLnMd2"
      },
      "source": [
        "* Simple to explain.\n",
        "* Model training and prediction are fast.\n",
        "* No tuning is required except regularization."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oxvYV8y3ndV5"
      },
      "source": [
        "### **Decision Tree:**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CC-50KtanVs3"
      },
      "source": [
        "* Decision trees are very intuitive and easy to explain.\n",
        "* They follow the same pattern of thinking that humans use when making decisions.\n",
        "* Decision trees are a common-sense technique to find the best solutions to problems with uncertainty."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KNQeWwUCnoaL"
      },
      "source": [
        "### **Random Forest:**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "guoP9clGntFa"
      },
      "source": [
        "* It is one of the most accurate learning algorithms available.\n",
        "* Random Forest consisits of multiple Decision Tress - Results from multiple trees are then merged to give best possible final outcome.\n",
        "* Random forests overcome several problems with decision trees like Reduction in overfitting."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JufCbWGNn4xi"
      },
      "source": [
        "**So, I want to approach from base model built using basic Linear Regression and then bring in more Sophisticated Algorithms - Decision Tree & Random Forest. It will give us good idea how Linear Regression performs against Decision Tree Regressor and Random Forest Regressor. Later, we will also approach with same algorithms on \"without PCA\" data. Finally, we'll evaluate both approaches we took and lay down recommended approach and algorithms.**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "o9vvpTOgmyeg"
      },
      "outputs": [],
      "source": [
        "#Passing in Transformed values as Predcitors\n",
        "\n",
        "X = transform\n",
        "y = np.log(Taxi_Time_df['trip_duration']).values"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "L2D-idVSoC7s"
      },
      "outputs": [],
      "source": [
        "#importing train test split & some important metrics\n",
        "\n",
        "from sklearn.model_selection import train_test_split, GridSearchCV, cross_val_score\n",
        "from sklearn.metrics import r2_score, mean_squared_log_error , mean_squared_error\n",
        "\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.30, random_state=10)\n",
        "X_train.shape, X_test.shape, y_train.shape, y_test.shape"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "CHkdQmDPJMvN"
      },
      "outputs": [],
      "source": [
        "\n",
        "# X = new_df\n",
        "# y = np.log(Taxi_Time_df['trip_duration']).values"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "doqI4-wo7wrX"
      },
      "source": [
        "### ML Model - 1"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OpvlyvHvJiqw"
      },
      "source": [
        "## Linear Regression"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "rFLXBKiB1nxb"
      },
      "outputs": [],
      "source": [
        "#implementing Linear regression\n",
        "from sklearn.linear_model import LinearRegression\n",
        "\n",
        "est_lr = LinearRegression()\n",
        "est_lr.fit(X_train, y_train)\n",
        "lr_pred = est_lr.predict(X_test)\n",
        "lr_pred"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "y_yOyu6foUcm"
      },
      "outputs": [],
      "source": [
        "#coeficients & intercept\n",
        "\n",
        "est_lr.intercept_, est_lr.coef_"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pisa1lI_odu4"
      },
      "source": [
        "Note: The Units / Values can change everytime you run the model freshly.\n",
        "\n",
        "Holding all other Principal Components fixed, a 1 unit increase in 1st PC is associated with a decrease of -0.12842633 in Trip Duration.\n",
        "\n",
        "Holding all other Principal Components fixed, a 1 unit increase in 2nd PC is associated with a increase of 0.18161527 in Trip Duration.\n",
        "\n",
        "Holding all other Principal Components fixed, a 1 unit increase in 3rd PC is associated with a decrease of -0.01200735 in Trip Duration.\n",
        "\n",
        "Holding all other Principal Components fixed, a 1 unit increase in 4th PC is associated with a increase of 0.01200735 in Trip Duration.\n",
        "\n",
        "Holding all other Principal Components fixed, a 1 unit increase in 5th PC is associated with a increase of 0.01654769 in Trip Duration.\n",
        "\n",
        "Holding all other Principal Components fixed, a 1 unit increase in 6th PC is associated with a increase of 0.08977729 in Trip Duration.\n",
        "\n",
        "Holding all other Principal Components fixed, a 1 unit increase in 7th PC is associated with a increase of 0.03316785 in Trip Duration.\n",
        "\n",
        "Holding all other Principal Components fixed, a 1 unit increase in 8th PC is associated with a increase of 0.01846044 in Trip Duration.\n",
        "\n",
        "Holding all other Principal Components fixed, a 1 unit increase in 9th PC is associated with a decrease of -0.00394832 in Trip Duration.\n",
        "\n",
        "Holding all other Principal Components fixed, a 1 unit increase in 10th PC is associated with a decrease of -0.00148052 in Trip Duration.\n",
        "\n",
        "Holding all other Principal Components fixed, a 1 unit increase in 11th PC is associated with a increase of 0.30265117 in Trip Duration.\n",
        "\n",
        "Holding all other Principal Components fixed, a 1 unit increase in 12th PC is associated with a decrease of -0.52439944 in Trip Duration.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "hb8-pfuk15cN"
      },
      "outputs": [],
      "source": [
        "#examining scores\n",
        "print (\"Training Score : \" , est_lr.score(X_train, y_train))\n",
        "\n",
        "print (\"Validation Score : \", est_lr.score(X_test, y_test))\n",
        "\n",
        "print (\"Cross Validation Score : \" , cross_val_score(est_lr, X_train, y_train, cv=5).mean())\n",
        "print (\"R2_Score : \", r2_score(lr_pred, y_test))\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "7hcsCY7A2BEL"
      },
      "outputs": [],
      "source": [
        "#prediction vs real data\n",
        "plt.style.use(\"seaborn-dark\")\n",
        "plt.figure(figsize=(15,8))\n",
        "plt.subplot(1,1,1)\n",
        "sns.distplot(y_test, kde=False, color=\"red\", label=\"Test\")\n",
        "\n",
        "plt.subplot(1,1,1)\n",
        "sns.distplot(lr_pred, kde=False, color=\"green\", label=\"Prediction\")\n",
        "plt.legend()\n",
        "plt.title(\"Test VS Prediction\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Uk3NHgMrpq0k"
      },
      "source": [
        "From the above Viz. we can clearly identify that the Linear Regression isn't performing good. The Actual Data (in Red) and Predicted values (in Green) are so much differing. We can conclude that Linear Regression doesn't seem like a right choice for Trip duration prediction."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "M29Q_18mp6kQ"
      },
      "source": [
        "## **Null RMSLE**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "IhiZIxll8h7M"
      },
      "outputs": [],
      "source": [
        "#null rmsle implementation\n",
        "from sklearn.metrics import mean_squared_log_error\n",
        "y_null = np.zeros_like(y_test, dtype=float)\n",
        "y_null.fill(y_test.mean())\n",
        "print (\"Null RMSLE : \", np.sqrt(mean_squared_log_error(y_test, y_null)))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AHjc--J-70Sk"
      },
      "source": [
        "### ML Model - 2"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lXaWC2F3KCta"
      },
      "source": [
        "## **Decision Tree**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "eFasOzMd9wpR"
      },
      "outputs": [],
      "source": [
        "#implementation of decision tree\n",
        "\n",
        "from sklearn.tree import DecisionTreeRegressor\n",
        "\n",
        "est_dt = DecisionTreeRegressor(criterion=\"mse\", max_depth=10)\n",
        "est_dt.fit(X_train, y_train)\n",
        "dt_pred = est_dt.predict(X_test)\n",
        "dt_pred"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "RNtnNqNO_B82"
      },
      "outputs": [],
      "source": [
        "#examining metrics\n",
        "\n",
        "print (\"Training Score : \" , est_dt.score(X_train, y_train))\n",
        "\n",
        "print (\"Validation Score : \", est_dt.score(X_test, y_test))\n",
        "\n",
        "print (\"Cross Validation Score : \" , cross_val_score(est_dt, X_train, y_train, cv=5).mean())\n",
        "\n",
        "print (\"R2_Score : \", r2_score(dt_pred, y_test))\n",
        "\n",
        "print (\"RMSLE : \", np.sqrt(mean_squared_log_error(dt_pred, y_test)))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LsUYNUVzqOYL"
      },
      "source": [
        "* Our Goal is to reduce the value of loss function (RMSLE) as much as possible considering NULL RMSLE into account, i.e, 0.1146"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "REEH4aKV_auN"
      },
      "outputs": [],
      "source": [
        "#prediction vs real data\n",
        "\n",
        "plt.figure(figsize=(15,8))\n",
        "plt.subplot(1,1,1)\n",
        "sns.distplot(y_test, kde=False, color=\"red\", label=\"Test\")\n",
        "\n",
        "plt.subplot(1,1,1)\n",
        "sns.distplot(dt_pred, kde=False, color=\"blue\", label=\"Prediction\")\n",
        "plt.legend()\n",
        "plt.title(\"Test VS Prediction\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8ogBDVhH_ZtX"
      },
      "source": [
        "* From the above Viz. we can clearly identify that the Decision Tree Algorithm is performing good. The Actual Data (in Red) and Predicted values (in Blue) are as close as possible. We can conclude that Decision Tree could be a good choice for Trip duration prediction."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6gOVMgWG7457"
      },
      "source": [
        "### ML Model - 3"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eFhbaQ5RL6cj"
      },
      "source": [
        "##**Random Forest**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "7mLVLkwKPjVt"
      },
      "outputs": [],
      "source": [
        "#random forest implementation\n",
        "from sklearn.ensemble import RandomForestRegressor\n",
        "est_rf = RandomForestRegressor(criterion=\"mse\", n_estimators=5, max_depth=10)\n",
        "est_rf.fit(X_train, y_train)\n",
        "rf_pred = est_rf.predict(X_test)\n",
        "rf_pred"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "gE5408rOPouj"
      },
      "outputs": [],
      "source": [
        "\n",
        "#examining metrics \n",
        "\n",
        "print (\"Training Score : \" , est_rf.score(X_train, y_train))\n",
        "\n",
        "print (\"Validation Score : \", est_rf.score(X_test, y_test))\n",
        "\n",
        "print (\"Cross Validation Score : \" , cross_val_score(est_rf, X_train, y_train, cv=5).mean())\n",
        "\n",
        "print (\"R2_Score : \", r2_score(rf_pred, y_test))\n",
        "\n",
        "print (\"RMSLE : \", np.sqrt(mean_squared_log_error(rf_pred, y_test)))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "IZL0iH8uPsV6"
      },
      "outputs": [],
      "source": [
        "#prediction vs real data\n",
        "\n",
        "plt.figure(figsize=(15,8))\n",
        "plt.subplot(1,1,1)\n",
        "sns.distplot(y_test, kde=False, color=\"black\", label=\"Test\")\n",
        "\n",
        "plt.subplot(1,1,1)\n",
        "sns.distplot(rf_pred, kde=False, color=\"green\", label=\"Prediction\")\n",
        "plt.legend()\n",
        "plt.title(\"Test VS Prediction\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qlD-NVu9qoas"
      },
      "source": [
        "* From the above Viz. we can clearly identify that the Random Forest Algorithm is also performing good. The Actual Data (in Black) and Predicted values (in Green) are as close as possible. We can conclude that Random Forest could be a good choice for Trip duration prediction.\n",
        "\n",
        "* Similarly, we can Hyper tune Random Forest to get the most out of it."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "33KF6l-_RHcg"
      },
      "source": [
        "\n",
        "##R2 Scores Evaluation"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QI1ynrMkq56N"
      },
      "source": [
        "* R2 Score or R-Squared is the proportion of the variance in the dependent variable that is predictable from the independent variable(s).\n",
        "\n",
        "* Best possible score is 1.0 and it can be negative (because the model can be arbitrarily worse). A constant model that always predicts the expected value of y, disregarding the input features, would get a R^2 score of 0.0."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "xEFWcF3JRELo"
      },
      "outputs": [],
      "source": [
        "#r2 score plot for all 3 models\n",
        "\n",
        "plt.figure(figsize=(10,7))\n",
        "r2 = pd.DataFrame({'Scores':np.array([r2_score(lr_pred, y_test), r2_score(dt_pred, y_test), r2_score(rf_pred, y_test)]), 'Model':np.array(['Linear Regression', 'Decison Tree', 'Random Forest'])})\n",
        "r2.set_index('Model').plot(kind=\"bar\", color=\"brown\")\n",
        "plt.axhline(y=0, color='g')\n",
        "plt.title(\"R2 Scores\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lnBGCBFdrASt"
      },
      "source": [
        "* Although , our Evaluation Metric isn't R2 Score but I'm just plotting them to check the Good Fit.\n",
        "\n",
        "* We're getting good fit score for Decision Tree and Random Forest , i.e, close to 1.0"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "O5GAEnfdL7Dk"
      },
      "source": [
        "##RMSLE Evaluation"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BKtTZnTtrJ6n"
      },
      "source": [
        "* RMSLE is a measure of how spread out these residuals are. In other words, it tells you how concentrated the data is around the line of best fit.\n",
        "* With RMSLE we explicitly know how much our predictions deviate.\n",
        "* Lower values of RMSLE indicate better fit with lesser LOSS."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "4IFLAdQjRRcs"
      },
      "outputs": [],
      "source": [
        "# #RMSLE plot\n",
        "plt.figure(figsize=(10,10))\n",
        "r2 = pd.DataFrame({'RMSLE':np.array([np.sqrt(mean_squared_log_error(dt_pred, y_test)), np.sqrt(mean_squared_log_error(rf_pred, y_test))]), 'Model':np.array(['Decison Tree', 'Random Forest'])})\n",
        "r2.set_index('Model').plot(kind=\"bar\", color=\"lightblue\", legend=False)\n",
        "plt.title(\"RMSLE - Lesser is Better\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LTGg1CAMrU0H"
      },
      "source": [
        "* Remember our NULL RMSLE : 0.1146 as a benchmark to beat.\n",
        "\n",
        "* We can observe from above Viz. that our Decision Tree model and Random Forest model are good performers. As, Random Forest is providing us reduced RMSLE, we can say that it's a model to Opt for."
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#**The Second Approach - Without PCA**"
      ],
      "metadata": {
        "id": "Ach0ArfdN5i4"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "* Another approach we could go with is without PCA, just Standard Scaling Dataset and applying our Algorithms.\n",
        "\n",
        "* The approach can give us better idea of what works better for us.\n",
        "\n",
        "* This approach might take great amount of computational resources and time, it will be good if we can run this on Googles Collaboratory, that will eliminate huge computational stress on our system as the program will be running on Cloud."
      ],
      "metadata": {
        "id": "-CClA-BBONjd"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "X = new_df\n",
        "y = np.log(Taxi_Time_df['trip_duration']).values"
      ],
      "metadata": {
        "id": "6tlL6waaN_of"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### ML Model - 1"
      ],
      "metadata": {
        "id": "39zg-75xQIm1"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **Linear regression**"
      ],
      "metadata": {
        "id": "PrhfMTX1ORGR"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#train test split\n",
        "\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.30, random_state=10)"
      ],
      "metadata": {
        "id": "A6KI1a-cOV9b"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#implenting linear regression\n",
        "\n",
        "est_lr = LinearRegression()\n",
        "est_lr.fit(X_train, y_train)\n",
        "lr_pred = est_lr.predict(X_test)\n",
        "lr_pred"
      ],
      "metadata": {
        "id": "DjN-Qv18OXXF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Intercept & Coef\n",
        "\n",
        "est_lr.intercept_, est_lr.coef_"
      ],
      "metadata": {
        "id": "8vcbiChNOdCG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "#Examining metrics\n",
        "\n",
        "print (\"Training Score : \" , est_lr.score(X_train, y_train))\n",
        "\n",
        "print (\"Validation Score : \", est_lr.score(X_test, y_test))\n",
        "\n",
        "print (\"Cross Validation Score : \" , cross_val_score(est_lr, X_train, y_train, cv=5).mean())\n",
        "\n",
        "print (\"R2_Score : \", r2_score(lr_pred, y_test))\n",
        "\n",
        "#print (\"RMSLE : \", np.sqrt(mean_squared_log_error(lr_pred, y_test)))"
      ],
      "metadata": {
        "id": "rI_vrPLKOgur"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#prediction vs validation data\n",
        "\n",
        "plt.figure(figsize=(15,8))\n",
        "plt.subplot(1,1,1)\n",
        "sns.distplot(y_test, kde=False, color=\"black\", label=\"Test\")\n",
        "\n",
        "plt.subplot(1,1,1)\n",
        "sns.distplot(lr_pred, kde=False, color=\"g\", label=\"Prediction\")\n",
        "plt.legend()\n",
        "plt.title(\"Test VS Prediction\")"
      ],
      "metadata": {
        "id": "JbLINyp-OkuY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "* Observations shows us that Linear Regression isn't performing well even with the second (without PCA) Approach."
      ],
      "metadata": {
        "id": "_xlvj-stOssk"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### ML Model - 2"
      ],
      "metadata": {
        "id": "edm3eGw2QMjM"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **Decision Tree**"
      ],
      "metadata": {
        "id": "tDfV0hgCOxUa"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#decision tree implementation\n",
        "\n",
        "est_dt = DecisionTreeRegressor(criterion=\"mse\", max_depth=10)\n",
        "est_dt.fit(X_train, y_train)\n",
        "dt_pred = est_dt.predict(X_test)\n",
        "dt_pred"
      ],
      "metadata": {
        "id": "Dki8lVHuOyOg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#examining metrics\n",
        "\n",
        "print (\"Training Score : \" , est_dt.score(X_train, y_train))\n",
        "\n",
        "print (\"Validation Score : \", est_dt.score(X_test, y_test))\n",
        "\n",
        "print (\"Cross Validation Score : \" , cross_val_score(est_dt, X_train, y_train, cv=5).mean())\n",
        "\n",
        "print (\"R2_Score : \", r2_score(dt_pred, y_test))\n",
        "\n",
        "print (\"RMSLE : \", np.sqrt(mean_squared_log_error(dt_pred, y_test)))"
      ],
      "metadata": {
        "id": "FfEN7Q7MO2Qa"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#prediction vs reality check\n",
        "\n",
        "plt.figure(figsize=(15,8))\n",
        "plt.subplot(1,1,1)\n",
        "sns.distplot(y_test, kde=False, color=\"black\", label=\"Test\")\n",
        "\n",
        "plt.subplot(1,1,1)\n",
        "sns.distplot(dt_pred, kde=False, color=\"cyan\", label=\"Prediction\")\n",
        "plt.legend()\n",
        "plt.title(\"Test VS Prediction\")"
      ],
      "metadata": {
        "id": "7GMSkVyYO6Yy"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "* Considering our Null RMSLE 0.1146, this model gave us loss of 0.0241, we can say it is good but not the acceptable, knowing the fact that we got RMSLE of 0.0241 in previous approach where we applied PCA."
      ],
      "metadata": {
        "id": "gETkJlQAO8-1"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### ML Model - 3"
      ],
      "metadata": {
        "id": "YGnI_dByQOsZ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##**Random Forest**"
      ],
      "metadata": {
        "id": "95f32FtvPCP8"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#implementation of forest algorithm\n",
        "\n",
        "from sklearn.ensemble import RandomForestRegressor\n",
        "\n",
        "est_rf = RandomForestRegressor(criterion=\"mse\", n_estimators=5, max_depth=10)\n",
        "est_rf.fit(X_train, y_train)\n",
        "rf_pred = est_rf.predict(X_test)\n",
        "rf_pred"
      ],
      "metadata": {
        "id": "f9JnkyvYPDFg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#examining metrics\n",
        "\n",
        "print (\"Training Score : \" , est_rf.score(X_train, y_train))\n",
        "\n",
        "print (\"Validation Score : \", est_rf.score(X_test, y_test))\n",
        "\n",
        "print (\"Cross Validation Score : \" , cross_val_score(est_rf, X_train, y_train, cv=5).mean())\n",
        "\n",
        "print (\"R2_Score : \", r2_score(rf_pred, y_test))\n",
        "\n",
        "print (\"RMSLE : \", np.sqrt(mean_squared_log_error(rf_pred, y_test)))"
      ],
      "metadata": {
        "id": "0pplwwJGPHHu"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#prediction vs reality check\n",
        "\n",
        "plt.figure(figsize=(15,8))\n",
        "plt.subplot(1,1,1)\n",
        "sns.distplot(y_test, kde=False, color=\"black\", label=\"Test\")\n",
        "\n",
        "plt.subplot(1,1,1)\n",
        "sns.distplot(rf_pred, kde=False, color=\"indigo\", label=\"Prediction\")\n",
        "plt.legend()\n",
        "plt.title(\"Test VS Prediction\")"
      ],
      "metadata": {
        "id": "wm7MSgIpPJ9e"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "* Again the loss value we got here is 0.0229 is good when tried to match with Decision Tree's RMSLE ,i.e, 0.0229. But still could be reduced by PCA Approach or maybe Hyper parameter tuning."
      ],
      "metadata": {
        "id": "HBWXUohHPNnM"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#r2 score plot for all 3 models\n",
        "\n",
        "plt.figure(figsize=(8,7))\n",
        "r2 = pd.DataFrame({'Scores':np.array([r2_score(lr_pred, y_test), r2_score(dt_pred, y_test), r2_score(rf_pred, y_test)]), 'Model':np.array(['Linear Regression', 'Decison Tree', 'Random Forest'])})\n",
        "r2.set_index('Model').plot(kind=\"bar\", color=\"maroon\")\n",
        "plt.axhline(y=0, color='g')\n",
        "plt.title(\"R2 Scores\")"
      ],
      "metadata": {
        "id": "y8IziFxFPTsu"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#RMSLE plot\n",
        "\n",
        "plt.figure(figsize=(8,7))\n",
        "r2 = pd.DataFrame({'RMSLE':np.array([np.sqrt(mean_squared_log_error(dt_pred, y_test)), np.sqrt(mean_squared_log_error(rf_pred, y_test))]), 'Model':np.array(['Decison Tree', 'Random Forest'])})\n",
        "r2.set_index('Model').plot(kind=\"bar\", color=\"skyblue\", legend=False)\n",
        "plt.title(\"RMSLE - Lesser is Better\")"
      ],
      "metadata": {
        "id": "0lUyK0iXPZCM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Q18kZlKP6CU9"
      },
      "source": [
        "##**What's better - Decision Tree or Random Forest ?**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FLyHLWF_6Jlw"
      },
      "source": [
        "\n",
        "* One problem that might occur with Decision Tree is that it can overfit.\n",
        "\n",
        "* Difference is - A random forest is a collection of decision trees.\n",
        "\n",
        "* A decision tree model considers all the features which makes it memorize everything, it gets overfitted on training data which couldn't predict well on unseen data.\n",
        "\n",
        "* A random forest chooses few number of rows at random and interprets results from all the Tress and combines it to get more accurate and stable final result."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "F0v-Rcnu6kSJ"
      },
      "source": [
        "##**Insights:**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xiyOF9F70UgQ"
      },
      "source": [
        "\n",
        "* Observed which taxi service provider is most Frequently used by New Yorkers.\n",
        "\n",
        "* Found out few trips which were of duration 528 Hours to 972 Hours, possibly Outliers.\n",
        "\n",
        "* With the help of Tableau, were able to make good use of Geographical Data provided in the Dataset to figure prominent Locations of Taxis pickup / dropoff points.\n",
        "\n",
        "* Also, found out some Trips of which pickup / dropoff point ended up somewhere in North Atlantic Sea.\n",
        "\n",
        "* Passenger count Analysis showed us that there were few trips with Zero Passengers.\n",
        "\n",
        "* Monthly trip analysis gives us a insight of Month  March and April marking the highest number of Trips while January marking lowest, possibly due to Snowfall."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_kgtDujD63xc"
      },
      "source": [
        "##**Recommended Approach:**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "18x4sVqX6v4D"
      },
      "source": [
        "* Apply Standard Scaling on the Dataset to Normalize the values.\n",
        "\n",
        "* Further, Apply PCA to reduce dimensions, as youll extract features from our primary DateTime Feature. Those additional features might lead our model to \n",
        "suffer from Curse of dimensionality and could drastically affect performance.\n",
        "\n",
        "* Pass the PCA Transformed data in our ML Regression Algorithms and Evaluate results."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EyNgTHvd2WFk"
      },
      "source": [
        "## **Future Work (Optional)**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lmy2xTVA7fMY"
      },
      "source": [
        "* Further, one can improve the model's performance using Hyper-Parameter Tuning.\n",
        "\n",
        "* Other ML Algorithms can be Tried.\n",
        "\n",
        "* One can take ANN approach."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-Kee-DAl2viO"
      },
      "source": [
        "### ***Congrats! Your model is successfully created and ready for deployment on a live server for a real user interaction !!!***"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gCX9965dhzqZ"
      },
      "source": [
        "# **Conclusion**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Fjb1IsQkh3yE"
      },
      "source": [
        "\n",
        "In this project we covered various aspects of the Machine learning development cycle. We observed that the data exploration and variable analysis is a very important aspect of the whole cycle and should be done for thorough understanding of the data. We also cleaned the data while exploring as there were some outliers which should be treated before feature engineering. Further we did feature engineering to filter and gather only the optimal features which are more significant and covered most of the variance in the dataset. Then finally we trained the models on the optimum featureset to get the results."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gIfDvo9L0UH2"
      },
      "source": [
        "### ***Hurrah! You have successfully completed your Machine Learning Capstone Project !!!***"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "collapsed_sections": [
        "beRrZCGUAJYm",
        "GNCClz6squbf",
        "ngyRFZaLYyqU",
        "OpvlyvHvJiqw",
        "_kgtDujD63xc",
        "EyNgTHvd2WFk",
        "-Kee-DAl2viO",
        "gCX9965dhzqZ",
        "gIfDvo9L0UH2"
      ],
      "provenance": [],
      "private_outputs": true,
      "include_colab_link": true
    },
    "gpuClass": "standard",
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}